#!/usr/bin/env python3
"""
Example script demonstrating batch processing of multiple YouTube video transcripts using ScrapeOps.
This script fetches transcripts for multiple videos and saves them to separate files.

Usage:
    python example_batch_processing.py <scrapeops_api_key> <video_id1> <video_id2> ...
"""
import sys
import os
import json
import logging
import time
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Any, Tuple, Optional

from youtube_transcript_api import YouTubeTranscriptApi, ScrapeOpsClient
from youtube_transcript_api.formatters import JSONFormatter, TextFormatter
from youtube_transcript_api._errors import (
    TranscriptsDisabled, 
    NoTranscriptFound,
    VideoUnavailable,
    IpBlocked,
    RequestBlocked
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Define output directory
OUTPUT_DIR = 'transcripts'


def setup_output_directory() -> None:
    """Create output directory if it doesn't exist."""
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        logger.info(f"Created output directory: {OUTPUT_DIR}")


def process_video(video_id: str, api_key: str) -> Tuple[str, Optional[Dict], Optional[str]]:
    """
    Process a single video to retrieve its transcript.
    
    Args:
        video_id: YouTube video ID
        api_key: ScrapeOps API key
        
    Returns:
        Tuple containing: video_id, transcript data (or None if failed), error message (or None if succeeded)
    """
    logger.info(f"Processing video: {video_id}")
    
    # Initialize the API with ScrapeOps
    ytt_api = YouTubeTranscriptApi(scrapeops_api_key=api_key)
    
    try:
        # Fetch transcript (try English first, then fall back to any available language)
        transcript = ytt_api.fetch(video_id, languages=['en', '*'])
        
        # Convert to raw data format
        transcript_data = transcript.to_raw_data()
        
        # Add metadata
        result = {
            'video_id': video_id,
            'language': transcript.language,
            'language_code': transcript.language_code,
            'is_generated': transcript.is_generated,
            'transcript': transcript_data
        }
        
        logger.info(f"Successfully retrieved transcript for {video_id}")
        return video_id, result, None
        
    except (TranscriptsDisabled, NoTranscriptFound, VideoUnavailable) as e:
        error_msg = f"{type(e).__name__}: {str(e)}"
        logger.warning(f"Could not retrieve transcript for {video_id}: {error_msg}")
        return video_id, None, error_msg
        
    except (IpBlocked, RequestBlocked) as e:
        error_msg = f"Blocking error: {type(e).__name__}: {str(e)}"
        logger.error(error_msg)
        # Re-raise blocking errors to handle them at a higher level
        raise
        
    except Exception as e:
        error_msg = f"Unexpected error: {type(e).__name__}: {str(e)}"
        logger.error(f"Error processing {video_id}: {error_msg}")
        return video_id, None, error_msg


def save_transcript(video_id: str, transcript_data: Dict) -> None:
    """
    Save transcript data to JSON and TXT files.
    
    Args:
        video_id: YouTube video ID
        transcript_data: Transcript data dictionary
    """
    # Save as JSON
    json_path = os.path.join(OUTPUT_DIR, f"{video_id}.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(transcript_data, f, indent=2, ensure_ascii=False)
    
    # Save as plain text
    text_path = os.path.join(OUTPUT_DIR, f"{video_id}.txt")
    with open(text_path, 'w', encoding='utf-8') as f:
        f.write(f"Transcript for YouTube video {video_id}\n")
        f.write(f"Language: {transcript_data['language']} ({transcript_data['language_code']})\n")
        f.write(f"Generated by YouTube: {transcript_data['is_generated']}\n\n")
        
        # Write time-coded transcript
        for item in transcript_data['transcript']:
            start_time = item['start']
            minutes = int(start_time // 60)
            seconds = int(start_time % 60)
            f.write(f"[{minutes:02d}:{seconds:02d}] {item['text']}\n")
    
    logger.info(f"Saved transcript for {video_id} as JSON and TXT")


def batch_process_videos(video_ids: List[str], api_key: str, max_workers: int = 3) -> Dict[str, Any]:
    """
    Process multiple videos in parallel using ThreadPoolExecutor.
    Implements basic rate limiting and error handling.
    
    Args:
        video_ids: List of YouTube video IDs
        api_key: ScrapeOps API key
        max_workers: Maximum number of parallel workers
        
    Returns:
        Dictionary with results summary
    """
    setup_output_directory()
    
    successful = []
    failed = {}
    
    # Using ThreadPoolExecutor for parallel processing with rate limiting
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all tasks
        future_to_video_id = {
            executor.submit(process_video, video_id, api_key): video_id 
            for video_id in video_ids
        }
        
        # Process results as they complete
        for future in future_to_video_id:
            video_id = future_to_video_id[future]
            try:
                result_video_id, transcript_data, error_msg = future.result()
                
                # If successful, save the transcript
                if transcript_data:
                    save_transcript(video_id, transcript_data)
                    successful.append(video_id)
                else:
                    failed[video_id] = error_msg
                    
                # Add a small delay between requests to avoid overwhelming the API
                time.sleep(0.5)
                
            except (IpBlocked, RequestBlocked) as e:
                # Handle blocking errors by slowing down
                logger.error(f"Blocking error for {video_id}: {e}. Pausing before continuing...")
                time.sleep(30)  # Pause for 30 seconds
                failed[video_id] = f"Blocking error: {str(e)}"
                
            except Exception as e:
                logger.error(f"Error processing future for {video_id}: {e}")
                failed[video_id] = f"Unexpected error: {str(e)}"
    
    # Generate summary report
    result = {
        'total': len(video_ids),
        'successful': len(successful),
        'failed': len(failed),
        'successful_ids': successful,
        'failed_ids': failed
    }
    
    return result


def main() -> None:
    """Main function to run the batch processing example."""
    # Check command line arguments
    if len(sys.argv) < 3:
        print(f"Usage: {sys.argv[0]} <scrapeops_api_key> <video_id1> <video_id2> ...")
        sys.exit(1)
        
    api_key = sys.argv[1]
    video_ids = sys.argv[2:]
    
    logger.info(f"Starting batch processing of {len(video_ids)} videos")
    
    # Process videos
    results = batch_process_videos(video_ids, api_key)
    
    # Print summary
    print("\nProcessing Summary:")
    print(f"Total videos processed: {results['total']}")
    print(f"Successfully retrieved: {results['successful']} videos")
    print(f"Failed to retrieve: {results['failed']} videos")
    
    if results['failed'] > 0:
        print("\nFailed videos and reasons:")
        for video_id, reason in results['failed_ids'].items():
            print(f"  - {video_id}: {reason}")
    
    print(f"\nTranscripts saved to the '{OUTPUT_DIR}' directory")


if __name__ == "__main__":
    main() 